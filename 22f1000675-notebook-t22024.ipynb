{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f0c0d8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.482316Z",
     "iopub.status.busy": "2024-07-30T11:29:20.481919Z",
     "iopub.status.idle": "2024-07-30T11:29:20.497327Z",
     "shell.execute_reply": "2024-07-30T11:29:20.496207Z"
    },
    "papermill": {
     "duration": 0.02609,
     "end_time": "2024-07-30T11:29:20.499852",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.473762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport numpy as np\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.preprocessing import OneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nimport matplotlib.pyplot as plt\\nimport os\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n\\n\\n# Load datasets\\ntrain = pd.read_csv(\\'/kaggle/input/crime-cast-forecasting-crime-categories/train.csv\\')\\ntest = pd.read_csv(\\'/kaggle/input/crime-cast-forecasting-crime-categories/test.csv\\')\\nsample_submission = pd.read_csv(\\'/kaggle/input/crime-cast-forecasting-crime-categories/sample.csv\\')\\n\\n# Display the first few rows of each DataFrame\\nprint(\"Train Data Sample:\")\\nprint(train.head())\\n\\nprint(\"\\nTest Data Sample:\")\\nprint(test.head())\\n\\nprint(\"\\nSample Submission Data Sample:\")\\nprint(sample_submission.head())'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/sample.csv')\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"Train Data Sample:\")\n",
    "print(train.head())\n",
    "\n",
    "print(\"\\nTest Data Sample:\")\n",
    "print(test.head())\n",
    "\n",
    "print(\"\\nSample Submission Data Sample:\")\n",
    "print(sample_submission.head())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7577cb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.514246Z",
     "iopub.status.busy": "2024-07-30T11:29:20.513829Z",
     "iopub.status.idle": "2024-07-30T11:29:20.520832Z",
     "shell.execute_reply": "2024-07-30T11:29:20.519684Z"
    },
    "papermill": {
     "duration": 0.017247,
     "end_time": "2024-07-30T11:29:20.523169",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.505922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"numeric_features = [\\n    'Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', \\n    'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code'\\n]\\n\\n# Plot histograms for the numeric features\\ntrain[numeric_features].hist(bins=30, figsize=(20, 15))\\n\\n# Set the title for the histograms\\nplt.suptitle('Histograms of Numeric Features')\\n\\n# Display the histograms\\nplt.show()\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''numeric_features = [\n",
    "    'Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', \n",
    "    'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code'\n",
    "]\n",
    "\n",
    "# Plot histograms for the numeric features\n",
    "train[numeric_features].hist(bins=30, figsize=(20, 15))\n",
    "\n",
    "# Set the title for the histograms\n",
    "plt.suptitle('Histograms of Numeric Features')\n",
    "\n",
    "# Display the histograms\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07126697",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.536778Z",
     "iopub.status.busy": "2024-07-30T11:29:20.536382Z",
     "iopub.status.idle": "2024-07-30T11:29:20.542904Z",
     "shell.execute_reply": "2024-07-30T11:29:20.541727Z"
    },
    "papermill": {
     "duration": 0.01619,
     "end_time": "2024-07-30T11:29:20.545327",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.529137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import seaborn as sns\\n# Customize pairplot\\nsns.set(style=\"ticks\", color_codes=True)\\ng = sns.pairplot(train[numeric_features], diag_kind=\\'kde\\', markers=\\'o\\', palette=None)\\n\\n# Add title\\nplt.suptitle(\\'Pairplot of Numeric Features\\', y=1.02, fontsize=16)\\n\\n# Adjust layout\\nplt.tight_layout()\\n\\n# Show plot\\nplt.show()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import seaborn as sns\n",
    "# Customize pairplot\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "g = sns.pairplot(train[numeric_features], diag_kind='kde', markers='o', palette=None)\n",
    "\n",
    "# Add title\n",
    "plt.suptitle('Pairplot of Numeric Features', y=1.02, fontsize=16)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a46025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.559848Z",
     "iopub.status.busy": "2024-07-30T11:29:20.559453Z",
     "iopub.status.idle": "2024-07-30T11:29:20.566642Z",
     "shell.execute_reply": "2024-07-30T11:29:20.565313Z"
    },
    "papermill": {
     "duration": 0.017225,
     "end_time": "2024-07-30T11:29:20.569160",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.551935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Set the style of seaborn\\nsns.set(style=\"whitegrid\")\\n\\n# Define the figure size\\nplt.figure(figsize=(10, 6))\\n\\n# Create box plots for numerical features\\nsns.boxplot(data=train[numeric_features])\\n\\n# Set title and labels\\nplt.title(\\'Box Plot of Numerical Features\\')\\nplt.xlabel(\\'Features\\')\\nplt.ylabel(\\'Values\\')\\n\\n# Rotate x-axis labels for better readability\\nplt.xticks(rotation=45)\\n\\n# Show the plot\\nplt.show()'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create box plots for numerical features\n",
    "sns.boxplot(data=train[numeric_features])\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Box Plot of Numerical Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7236420e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.586823Z",
     "iopub.status.busy": "2024-07-30T11:29:20.586298Z",
     "iopub.status.idle": "2024-07-30T11:29:20.597916Z",
     "shell.execute_reply": "2024-07-30T11:29:20.596716Z"
    },
    "papermill": {
     "duration": 0.02448,
     "end_time": "2024-07-30T11:29:20.600518",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.576038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport numpy as np\\nimport string\\nimport re\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\\n# Define text column\\ntext_column = \\'Status_Description\\'  # Adjust this if a different column contains text data\\n\\n# Define text_data_train\\ntext_data_train = train[text_column].fillna(\\'\\')\\n\\n# Function to preprocess text\\ndef preprocess_text(text):\\n    # Convert text to lowercase\\n    text = text.lower()\\n    # Remove punctuation\\n    text = text.translate(str.maketrans(\\'\\', \\'\\', string.punctuation))\\n    # Remove digits\\n    text = re.sub(r\\'\\\\d+\\', \\'\\', text)\\n    # Tokenization (split the text into words)\\n    tokens = text.split()\\n    # Remove stop words\\n    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\\n    # Join tokens back into text\\n    processed_text = \\' \\'.join(tokens)\\n    return processed_text\\n\\n# Apply preprocessing to text data\\ntext_data_train_preprocessed = text_data_train.apply(preprocess_text)\\n\\n# CountVectorizer\\ncount_vectorizer = CountVectorizer(max_features=280, max_df=0.9, min_df=2)\\n\\n# Fit and transform on preprocessed train data\\ncount_data_train = count_vectorizer.fit_transform(text_data_train_preprocessed)\\ncount_df_train = pd.DataFrame(count_data_train.toarray(), columns=count_vectorizer.get_feature_names_out())\\n\\n# Compute additional features for train data\\nword_count = text_data_train.str.split().apply(len)\\nsentence_count = text_data_train.str.split(r\\'[.!?]\\').apply(len)\\npunctuation_count = text_data_train.str.count(r\\'[,.!?]\\')\\ncapitalized_word_count = text_data_train.str.findall(r\\'\\x08[A-Z]\\\\w*\\x08\\').apply(len)\\nexclamation_count = text_data_train.str.count(r\\'!\\')\\nquestion_count = text_data_train.str.count(r\\'\\\\?\\')\\n\\n# Define function to remove outliers\\ndef remove_outliers(data, column, threshold=3):\\n    \"\"\"\\n    Remove outliers from a column of data using the z-score method.\\n    \\n    Parameters:\\n        data (pandas.DataFrame): The DataFrame containing the column.\\n        column (str): The name of the column from which outliers are to be removed.\\n        threshold (float): The threshold for considering a data point as an outlier based on its z-score.\\n                           Default is 3, which is a common threshold.\\n    \\n    Returns:\\n        pandas.DataFrame: The DataFrame with outliers removed from the specified column.\\n    \"\"\"\\n    # Calculate mean and standard deviation of the column\\n    mean = data[column].mean()\\n    std_dev = data[column].std()\\n    \\n    # Calculate z-scores for each data point in the column\\n    z_scores = np.abs((data[column] - mean) / std_dev)\\n    \\n    # Identify outliers based on z-scores\\n    outliers = data[z_scores > threshold]\\n    \\n    # Remove outliers from the DataFrame\\n    cleaned_data = data[z_scores <= threshold]\\n    \\n    return cleaned_data\\n\\n# List of numerical features to be used\\nnumeric_features = [\\n    \\'Latitude\\', \\'Longitude\\', \\'Time_Occurred\\', \\'Area_ID\\', \\n    \\'Reporting_District_no\\', \\'Victim_Age\\', \\'Premise_Code\\', \\'Weapon_Used_Code\\'\\n]\\n\\n# Concatenate count features with the original DataFrame for train data\\ntrain_final = pd.concat([train[numeric_features], count_df_train, \\n                         word_count.rename(\\'WordCount\\'), \\n                         sentence_count.rename(\\'SentenceCount\\'), \\n                         punctuation_count.rename(\\'PunctuationCount\\'),\\n                         capitalized_word_count.rename(\\'CapitalizedWordCount\\'),\\n                         exclamation_count.rename(\\'ExclamationCount\\'),\\n                         question_count.rename(\\'QuestionCount\\')], axis=1)\\n\\n# Remove outliers from the numerical features\\nfor feature in numeric_features:\\n    train_final = remove_outliers(train_final, feature)\\n\\n# Save train data along with CountVectorizer to a CSV file if needed\\ntrain_final.to_csv(\\'train_final.csv\\', index=False)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# Define text column\n",
    "text_column = 'Status_Description'  # Adjust this if a different column contains text data\n",
    "\n",
    "# Define text_data_train\n",
    "text_data_train = train[text_column].fillna('')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization (split the text into words)\n",
    "    tokens = text.split()\n",
    "    # Remove stop words\n",
    "    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    # Join tokens back into text\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing to text data\n",
    "text_data_train_preprocessed = text_data_train.apply(preprocess_text)\n",
    "\n",
    "# CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features=280, max_df=0.9, min_df=2)\n",
    "\n",
    "# Fit and transform on preprocessed train data\n",
    "count_data_train = count_vectorizer.fit_transform(text_data_train_preprocessed)\n",
    "count_df_train = pd.DataFrame(count_data_train.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Compute additional features for train data\n",
    "word_count = text_data_train.str.split().apply(len)\n",
    "sentence_count = text_data_train.str.split(r'[.!?]').apply(len)\n",
    "punctuation_count = text_data_train.str.count(r'[,.!?]')\n",
    "capitalized_word_count = text_data_train.str.findall(r'\\b[A-Z]\\w*\\b').apply(len)\n",
    "exclamation_count = text_data_train.str.count(r'!')\n",
    "question_count = text_data_train.str.count(r'\\?')\n",
    "\n",
    "# Define function to remove outliers\n",
    "def remove_outliers(data, column, threshold=3):\n",
    "    \"\"\"\n",
    "    Remove outliers from a column of data using the z-score method.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pandas.DataFrame): The DataFrame containing the column.\n",
    "        column (str): The name of the column from which outliers are to be removed.\n",
    "        threshold (float): The threshold for considering a data point as an outlier based on its z-score.\n",
    "                           Default is 3, which is a common threshold.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with outliers removed from the specified column.\n",
    "    \"\"\"\n",
    "    # Calculate mean and standard deviation of the column\n",
    "    mean = data[column].mean()\n",
    "    std_dev = data[column].std()\n",
    "    \n",
    "    # Calculate z-scores for each data point in the column\n",
    "    z_scores = np.abs((data[column] - mean) / std_dev)\n",
    "    \n",
    "    # Identify outliers based on z-scores\n",
    "    outliers = data[z_scores > threshold]\n",
    "    \n",
    "    # Remove outliers from the DataFrame\n",
    "    cleaned_data = data[z_scores <= threshold]\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "# List of numerical features to be used\n",
    "numeric_features = [\n",
    "    'Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', \n",
    "    'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code'\n",
    "]\n",
    "\n",
    "# Concatenate count features with the original DataFrame for train data\n",
    "train_final = pd.concat([train[numeric_features], count_df_train, \n",
    "                         word_count.rename('WordCount'), \n",
    "                         sentence_count.rename('SentenceCount'), \n",
    "                         punctuation_count.rename('PunctuationCount'),\n",
    "                         capitalized_word_count.rename('CapitalizedWordCount'),\n",
    "                         exclamation_count.rename('ExclamationCount'),\n",
    "                         question_count.rename('QuestionCount')], axis=1)\n",
    "\n",
    "# Remove outliers from the numerical features\n",
    "for feature in numeric_features:\n",
    "    train_final = remove_outliers(train_final, feature)\n",
    "\n",
    "# Save train data along with CountVectorizer to a CSV file if needed\n",
    "train_final.to_csv('train_final.csv', index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6184da36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.615776Z",
     "iopub.status.busy": "2024-07-30T11:29:20.615348Z",
     "iopub.status.idle": "2024-07-30T11:29:20.622548Z",
     "shell.execute_reply": "2024-07-30T11:29:20.621329Z"
    },
    "papermill": {
     "duration": 0.018126,
     "end_time": "2024-07-30T11:29:20.625176",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.607050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Define text data for testing (assuming 'test' DataFrame is defined)\\n# Replace 'Recipe_Review' with 'Crime_Category' in the test DataFrame\\ntext_data_test = test['Status_Description'].fillna('')\\n\\n# Apply preprocessing to text data for testing\\ntext_data_test_preprocessed = text_data_test.apply(preprocess_text)\\n\\n# Transform test data using the previously fitted CountVectorizer\\ncount_data_test = count_vectorizer.transform(text_data_test_preprocessed)\\ncount_df_test = pd.DataFrame(count_data_test.toarray(), columns=count_vectorizer.get_feature_names_out())\\n\\n# If you want to save or further process count_df_test, you can do so here\\n# For example, to save the transformed data to a CSV file:\\n# count_df_test.to_csv('test_transformed.csv', index=False)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Define text data for testing (assuming 'test' DataFrame is defined)\n",
    "# Replace 'Recipe_Review' with 'Crime_Category' in the test DataFrame\n",
    "text_data_test = test['Status_Description'].fillna('')\n",
    "\n",
    "# Apply preprocessing to text data for testing\n",
    "text_data_test_preprocessed = text_data_test.apply(preprocess_text)\n",
    "\n",
    "# Transform test data using the previously fitted CountVectorizer\n",
    "count_data_test = count_vectorizer.transform(text_data_test_preprocessed)\n",
    "count_df_test = pd.DataFrame(count_data_test.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# If you want to save or further process count_df_test, you can do so here\n",
    "# For example, to save the transformed data to a CSV file:\n",
    "# count_df_test.to_csv('test_transformed.csv', index=False)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8a6a03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.640413Z",
     "iopub.status.busy": "2024-07-30T11:29:20.640011Z",
     "iopub.status.idle": "2024-07-30T11:29:20.647671Z",
     "shell.execute_reply": "2024-07-30T11:29:20.646527Z"
    },
    "papermill": {
     "duration": 0.017793,
     "end_time": "2024-07-30T11:29:20.649835",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.632042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Define the list of numerical features to include in the final DataFrame\\nnumerical_features = [\\n    'Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', \\n    'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code'\\n]\\n\\n# Ensure the test DataFrame includes all the necessary numerical features\\ntest_final = test[numerical_features].copy()\\n\\n# Transform the numerical features into a DataFrame\\ncount_data_test = count_vectorizer.transform(text_data_test_preprocessed)\\ncount_df_test = pd.DataFrame(count_data_test.toarray(), columns=count_vectorizer.get_feature_names_out())\\n\\n# Concatenate the CountVectorizer features with the numerical features\\ntest_final = pd.concat([test_final, count_df_test], axis=1)\\n\\n# Save the final test DataFrame to a CSV file if needed\\ntest_final.to_csv('test_final.csv', index=False)\\n\\n# Display the head and shape of the final DataFrame for test data\\nprint(test_final.head())\\nprint(test_final.shape)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Define the list of numerical features to include in the final DataFrame\n",
    "numerical_features = [\n",
    "    'Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', \n",
    "    'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code'\n",
    "]\n",
    "\n",
    "# Ensure the test DataFrame includes all the necessary numerical features\n",
    "test_final = test[numerical_features].copy()\n",
    "\n",
    "# Transform the numerical features into a DataFrame\n",
    "count_data_test = count_vectorizer.transform(text_data_test_preprocessed)\n",
    "count_df_test = pd.DataFrame(count_data_test.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the CountVectorizer features with the numerical features\n",
    "test_final = pd.concat([test_final, count_df_test], axis=1)\n",
    "\n",
    "# Save the final test DataFrame to a CSV file if needed\n",
    "test_final.to_csv('test_final.csv', index=False)\n",
    "\n",
    "# Display the head and shape of the final DataFrame for test data\n",
    "print(test_final.head())\n",
    "print(test_final.shape)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a87dc2a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.665615Z",
     "iopub.status.busy": "2024-07-30T11:29:20.665187Z",
     "iopub.status.idle": "2024-07-30T11:29:20.672880Z",
     "shell.execute_reply": "2024-07-30T11:29:20.671696Z"
    },
    "papermill": {
     "duration": 0.018719,
     "end_time": "2024-07-30T11:29:20.675480",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.656761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Set the style of seaborn\\nsns.set(style=\"whitegrid\")\\n\\n# Plot top Count Vectorizer features\\nplt.figure(figsize=(15, 10))\\nsns.barplot(x=count_df_train.columns[:40], y=count_df_train.iloc[:, :40].sum(), palette=\\'viridis\\')\\nplt.title(\\'Top 5 Most Common Words (Count Vectorizer)\\')\\nplt.xlabel(\\'Words\\')\\nplt.ylabel(\\'Frequency\\')\\nplt.xticks(rotation=45)\\nplt.show()\\n\\n# Plot numerical features\\n# Select numerical features from the DataFrame\\nnumerical_features = [\\n    \\'Latitude\\', \\'Longitude\\', \\'Time_Occurred\\', \\'Area_ID\\', \\n    \\'Reporting_District_no\\', \\'Victim_Age\\', \\'Premise_Code\\', \\'Weapon_Used_Code\\'\\n]\\n\\n# Plot distributions of numerical features\\nplt.figure(figsize=(20, 15))\\nfor i, feature in enumerate(numerical_features, 1):\\n    plt.subplot(3, 3, i)\\n    sns.histplot(train_final[feature].dropna(), kde=True, bins=30, color=\\'blue\\')\\n    plt.title(f\\'Distribution of {feature}\\')\\n    plt.xlabel(feature)\\n    plt.ylabel(\\'Frequency\\')\\n\\nplt.tight_layout()\\nplt.show()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Set the style of seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot top Count Vectorizer features\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x=count_df_train.columns[:40], y=count_df_train.iloc[:, :40].sum(), palette='viridis')\n",
    "plt.title('Top 5 Most Common Words (Count Vectorizer)')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot numerical features\n",
    "# Select numerical features from the DataFrame\n",
    "numerical_features = [\n",
    "    'Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', \n",
    "    'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code'\n",
    "]\n",
    "\n",
    "# Plot distributions of numerical features\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(train_final[feature].dropna(), kde=True, bins=30, color='blue')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3a24c",
   "metadata": {
    "papermill": {
     "duration": 0.006865,
     "end_time": "2024-07-30T11:29:20.689495",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.682630",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model 1: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dafe52ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.707200Z",
     "iopub.status.busy": "2024-07-30T11:29:20.706726Z",
     "iopub.status.idle": "2024-07-30T11:29:20.715815Z",
     "shell.execute_reply": "2024-07-30T11:29:20.714672Z"
    },
    "papermill": {
     "duration": 0.020723,
     "end_time": "2024-07-30T11:29:20.718112",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.697389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from imblearn.pipeline import Pipeline as imPipeline\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\\nfrom sklearn.metrics import accuracy_score, classification_report\\n\\n# Define feature columns and target column\\nfeature_columns = ['Location', 'Cross_Street', 'Latitude', 'Longitude', 'Date_Reported', 'Date_Occurred', 'Time_Occurred', 'Area_ID', 'Area_Name', 'Reporting_District_no', 'Part 1-2', 'Modus_Operandi', 'Victim_Age', 'Victim_Sex', 'Victim_Descent', 'Premise_Code', 'Premise_Description', 'Weapon_Used_Code', 'Weapon_Description', 'Status', 'Status_Description']\\ntarget_column = 'Crime_Category'\\n\\n# Separate features and target\\nX = train[feature_columns]\\ny = train[target_column]\\n\\n# Define preprocessor\\nnumeric_features = [\\n    'Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', \\n    'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code'\\n]\\ncategorical_features = list(set(feature_columns) - set(numeric_features))\\n\\nnumeric_transformer = imPipeline(steps=[\\n    ('imputer', SimpleImputer(strategy='mean')),\\n    ('scaler', StandardScaler())])\\n\\ncategorical_transformer = imPipeline(steps=[\\n    ('imputer', SimpleImputer(strategy='most_frequent')),\\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\\n\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        ('num', numeric_transformer, numeric_features),\\n        ('cat', categorical_transformer, categorical_features)])\\n\\n# Define the model with class weights\\nmodel = RandomForestClassifier(n_estimators=500, class_weight='balanced', random_state=42)\\n\\n# Adjust hyperparameter grid\\nparam_distributions = {\\n    'classifier__max_depth': [20, 30, 40],\\n    'classifier__min_samples_split': [2, 5],\\n    'classifier__min_samples_leaf': [1, 2],\\n    'classifier__bootstrap': [True]\\n}\\n\\n# Create pipeline without SMOTE\\npipeline = imPipeline(steps=[\\n    ('preprocessor', preprocessor),\\n    ('classifier', model)\\n])\\n\\n# Perform a train-test split\\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Hyperparameter tuning with RandomizedSearchCV\\nrandom_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=10, cv=5, verbose=2, n_jobs=-1, random_state=42)\\nrandom_search.fit(X_train, y_train)\\n\\n# Best model from RandomizedSearchCV\\nbest_model = random_search.best_estimator_\\n\\n# Evaluate the model\\ny_pred = best_model.predict(X_val)\\nprint('Validation Accuracy:', accuracy_score(y_val, y_pred))\\nprint('Classification Report:')\\nprint(classification_report(y_val, y_pred))\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from imblearn.pipeline import Pipeline as imPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define feature columns and target column\n",
    "feature_columns = ['Location', 'Cross_Street', 'Latitude', 'Longitude', 'Date_Reported', 'Date_Occurred', 'Time_Occurred', 'Area_ID', 'Area_Name', 'Reporting_District_no', 'Part 1-2', 'Modus_Operandi', 'Victim_Age', 'Victim_Sex', 'Victim_Descent', 'Premise_Code', 'Premise_Description', 'Weapon_Used_Code', 'Weapon_Description', 'Status', 'Status_Description']\n",
    "target_column = 'Crime_Category'\n",
    "\n",
    "# Separate features and target\n",
    "X = train[feature_columns]\n",
    "y = train[target_column]\n",
    "\n",
    "# Define preprocessor\n",
    "numeric_features = [\n",
    "    'Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', \n",
    "    'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code'\n",
    "]\n",
    "categorical_features = list(set(feature_columns) - set(numeric_features))\n",
    "\n",
    "numeric_transformer = imPipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = imPipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Define the model with class weights\n",
    "model = RandomForestClassifier(n_estimators=500, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Adjust hyperparameter grid\n",
    "param_distributions = {\n",
    "    'classifier__max_depth': [20, 30, 40],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2],\n",
    "    'classifier__bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Create pipeline without SMOTE\n",
    "pipeline = imPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Perform a train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=10, cv=5, verbose=2, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model from RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_model.predict(X_val)\n",
    "print('Validation Accuracy:', accuracy_score(y_val, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val, y_pred))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a11c18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.734817Z",
     "iopub.status.busy": "2024-07-30T11:29:20.734415Z",
     "iopub.status.idle": "2024-07-30T11:29:20.742598Z",
     "shell.execute_reply": "2024-07-30T11:29:20.741485Z"
    },
    "papermill": {
     "duration": 0.0195,
     "end_time": "2024-07-30T11:29:20.745180",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.725680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Check the columns in test_final\\nprint(\"Columns in test_final:\", test_final.columns)\\n\\n# Ensure all required columns are present in test_final\\nfor col in feature_columns:\\n    if col not in test_final.columns:\\n        print(f\"Warning: Column \\'{col}\\' is missing in test_final. Adding it with NaN values.\")\\n        test_final[col] = None  # Add missing columns with NaN values\\n\\n# Make predictions on the test_final set\\n# Note: Ensure the test_final DataFrame has all feature columns with the correct names and types\\ntest_final_preprocessed = best_model.named_steps[\\'preprocessor\\'].transform(test_final[feature_columns])\\ntest_predictions = best_model.named_steps[\\'classifier\\'].predict(test_final_preprocessed)\\n\\n# Create a sequential ID column for test_final\\ntest_final[\\'ID\\'] = test_final.index + 1  # Creates IDs starting from 1\\n\\n# Create the submission DataFrame\\nsubmission = pd.DataFrame({\\n    \\'ID\\': test_final[\\'ID\\'],\\n    \\'Crime_Category\\': test_predictions\\n})\\n\\n# Sumission\\nsubmission.to_csv(\\'submission.csv\\', index=False)\\n\\n# Confirm the file has been saved\\nprint(f\"Submission file saved\")'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Check the columns in test_final\n",
    "print(\"Columns in test_final:\", test_final.columns)\n",
    "\n",
    "# Ensure all required columns are present in test_final\n",
    "for col in feature_columns:\n",
    "    if col not in test_final.columns:\n",
    "        print(f\"Warning: Column '{col}' is missing in test_final. Adding it with NaN values.\")\n",
    "        test_final[col] = None  # Add missing columns with NaN values\n",
    "\n",
    "# Make predictions on the test_final set\n",
    "# Note: Ensure the test_final DataFrame has all feature columns with the correct names and types\n",
    "test_final_preprocessed = best_model.named_steps['preprocessor'].transform(test_final[feature_columns])\n",
    "test_predictions = best_model.named_steps['classifier'].predict(test_final_preprocessed)\n",
    "\n",
    "# Create a sequential ID column for test_final\n",
    "test_final['ID'] = test_final.index + 1  # Creates IDs starting from 1\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_final['ID'],\n",
    "    'Crime_Category': test_predictions\n",
    "})\n",
    "\n",
    "# Sumission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Confirm the file has been saved\n",
    "print(f\"Submission file saved\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4693cd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.763032Z",
     "iopub.status.busy": "2024-07-30T11:29:20.762639Z",
     "iopub.status.idle": "2024-07-30T11:29:20.767544Z",
     "shell.execute_reply": "2024-07-30T11:29:20.766402Z"
    },
    "papermill": {
     "duration": 0.016015,
     "end_time": "2024-07-30T11:29:20.770062",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.754047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59365848",
   "metadata": {
    "papermill": {
     "duration": 0.007246,
     "end_time": "2024-07-30T11:29:20.785355",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.778109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model 2: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1be77f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.802955Z",
     "iopub.status.busy": "2024-07-30T11:29:20.802576Z",
     "iopub.status.idle": "2024-07-30T11:29:20.812012Z",
     "shell.execute_reply": "2024-07-30T11:29:20.810651Z"
    },
    "papermill": {
     "duration": 0.020984,
     "end_time": "2024-07-30T11:29:20.814592",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.793608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import os\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split, GridSearchCV\\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\nfor dirname, _, filenames in os.walk(\\'/kaggle/input\\'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\\n# Load datasets\\ntrain = pd.read_csv(\\'/kaggle/input/crime-cast-forecasting-crime-categories/train.csv\\')\\ntest = pd.read_csv(\\'/kaggle/input/crime-cast-forecasting-crime-categories/test.csv\\')\\nsample_submission = pd.read_csv(\\'/kaggle/input/crime-cast-forecasting-crime-categories/sample.csv\\')\\n\\n# Handle missing values\\ntrain.fillna({\\n    \\'Location\\': \\'Unknown\\',\\n    \\'Cross_Street\\': \\'Unknown\\',\\n    \\'Latitude\\': train[\\'Latitude\\'].median(),\\n    \\'Longitude\\': train[\\'Longitude\\'].median(),\\n    \\'Date_Reported\\': \\'Unknown\\',\\n    \\'Date_Occurred\\': \\'Unknown\\',\\n    \\'Time_Occurred\\': train[\\'Time_Occurred\\'].median(),\\n    \\'Area_ID\\': train[\\'Area_ID\\'].median(),\\n    \\'Area_Name\\': \\'Unknown\\',\\n    \\'Reporting_District_no\\': train[\\'Reporting_District_no\\'].median(),\\n    \\'Part 1-2\\': \\'Unknown\\',\\n    \\'Modus_Operandi\\': \\'Unknown\\',\\n    \\'Victim_Age\\': train[\\'Victim_Age\\'].median(),\\n    \\'Victim_Sex\\': \\'Unknown\\',\\n    \\'Victim_Descent\\': \\'Unknown\\',\\n    \\'Premise_Code\\': train[\\'Premise_Code\\'].median(),\\n    \\'Premise_Description\\': \\'Unknown\\',\\n    \\'Weapon_Used_Code\\': train[\\'Weapon_Used_Code\\'].median(),\\n    \\'Weapon_Description\\': \\'Unknown\\',\\n    \\'Status\\': \\'Unknown\\',\\n    \\'Status_Description\\': \\'Unknown\\'\\n}, inplace=True)\\n\\ntest.fillna({\\n    \\'Location\\': \\'Unknown\\',\\n    \\'Cross_Street\\': \\'Unknown\\',\\n    \\'Latitude\\': test[\\'Latitude\\'].median(),\\n    \\'Longitude\\': test[\\'Longitude\\'].median(),\\n    \\'Date_Reported\\': \\'Unknown\\',\\n    \\'Date_Occurred\\': \\'Unknown\\',\\n    \\'Time_Occurred\\': test[\\'Time_Occurred\\'].median(),\\n    \\'Area_ID\\': test[\\'Area_ID\\'].median(),\\n    \\'Area_Name\\': \\'Unknown\\',\\n    \\'Reporting_District_no\\': test[\\'Reporting_District_no\\'].median(),\\n    \\'Part 1-2\\': \\'Unknown\\',\\n    \\'Modus_Operandi\\': \\'Unknown\\',\\n    \\'Victim_Age\\': test[\\'Victim_Age\\'].median(),\\n    \\'Victim_Sex\\': \\'Unknown\\',\\n    \\'Victim_Descent\\': \\'Unknown\\',\\n    \\'Premise_Code\\': test[\\'Premise_Code\\'].median(),\\n    \\'Premise_Description\\': \\'Unknown\\',\\n    \\'Weapon_Used_Code\\': test[\\'Weapon_Used_Code\\'].median(),\\n    \\'Weapon_Description\\': \\'Unknown\\',\\n    \\'Status\\': \\'Unknown\\',\\n    \\'Status_Description\\': \\'Unknown\\'\\n}, inplace=True)\\n\\n# Display the first few rows of each DataFrame\\nprint(\"Train Data Sample:\")\\nprint(train.head())\\n\\nprint(\"\\nTest Data Sample:\")\\nprint(test.head())\\n\\nprint(\"\\nSample Submission Data Sample:\")\\nprint(sample_submission.head())'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/sample.csv')\n",
    "\n",
    "# Handle missing values\n",
    "train.fillna({\n",
    "    'Location': 'Unknown',\n",
    "    'Cross_Street': 'Unknown',\n",
    "    'Latitude': train['Latitude'].median(),\n",
    "    'Longitude': train['Longitude'].median(),\n",
    "    'Date_Reported': 'Unknown',\n",
    "    'Date_Occurred': 'Unknown',\n",
    "    'Time_Occurred': train['Time_Occurred'].median(),\n",
    "    'Area_ID': train['Area_ID'].median(),\n",
    "    'Area_Name': 'Unknown',\n",
    "    'Reporting_District_no': train['Reporting_District_no'].median(),\n",
    "    'Part 1-2': 'Unknown',\n",
    "    'Modus_Operandi': 'Unknown',\n",
    "    'Victim_Age': train['Victim_Age'].median(),\n",
    "    'Victim_Sex': 'Unknown',\n",
    "    'Victim_Descent': 'Unknown',\n",
    "    'Premise_Code': train['Premise_Code'].median(),\n",
    "    'Premise_Description': 'Unknown',\n",
    "    'Weapon_Used_Code': train['Weapon_Used_Code'].median(),\n",
    "    'Weapon_Description': 'Unknown',\n",
    "    'Status': 'Unknown',\n",
    "    'Status_Description': 'Unknown'\n",
    "}, inplace=True)\n",
    "\n",
    "test.fillna({\n",
    "    'Location': 'Unknown',\n",
    "    'Cross_Street': 'Unknown',\n",
    "    'Latitude': test['Latitude'].median(),\n",
    "    'Longitude': test['Longitude'].median(),\n",
    "    'Date_Reported': 'Unknown',\n",
    "    'Date_Occurred': 'Unknown',\n",
    "    'Time_Occurred': test['Time_Occurred'].median(),\n",
    "    'Area_ID': test['Area_ID'].median(),\n",
    "    'Area_Name': 'Unknown',\n",
    "    'Reporting_District_no': test['Reporting_District_no'].median(),\n",
    "    'Part 1-2': 'Unknown',\n",
    "    'Modus_Operandi': 'Unknown',\n",
    "    'Victim_Age': test['Victim_Age'].median(),\n",
    "    'Victim_Sex': 'Unknown',\n",
    "    'Victim_Descent': 'Unknown',\n",
    "    'Premise_Code': test['Premise_Code'].median(),\n",
    "    'Premise_Description': 'Unknown',\n",
    "    'Weapon_Used_Code': test['Weapon_Used_Code'].median(),\n",
    "    'Weapon_Description': 'Unknown',\n",
    "    'Status': 'Unknown',\n",
    "    'Status_Description': 'Unknown'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"Train Data Sample:\")\n",
    "print(train.head())\n",
    "\n",
    "print(\"\\nTest Data Sample:\")\n",
    "print(test.head())\n",
    "\n",
    "print(\"\\nSample Submission Data Sample:\")\n",
    "print(sample_submission.head())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "776691c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.831695Z",
     "iopub.status.busy": "2024-07-30T11:29:20.831036Z",
     "iopub.status.idle": "2024-07-30T11:29:20.839129Z",
     "shell.execute_reply": "2024-07-30T11:29:20.837910Z"
    },
    "papermill": {
     "duration": 0.019292,
     "end_time": "2024-07-30T11:29:20.841480",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.822188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Encode categorical variables\\ncategorical_features = ['Location', 'Cross_Street', 'Date_Reported', 'Date_Occurred', 'Area_Name', 'Part 1-2', 'Modus_Operandi', 'Victim_Sex', 'Victim_Descent', 'Premise_Description', 'Weapon_Description', 'Status', 'Status_Description']\\nnumeric_features = ['Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', 'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code']\\n\\n# Create preprocessing pipelines for numeric and categorical data\\nnumeric_transformer = Pipeline(steps=[\\n    ('scaler', StandardScaler())\\n])\\n\\ncategorical_transformer = Pipeline(steps=[\\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\\n])\\n\\n# Combine preprocessing steps\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        ('num', numeric_transformer, numeric_features),\\n        ('cat', categorical_transformer, categorical_features)\\n    ])\\n\\n# Define the model pipeline\\nmodel = Pipeline(steps=[\\n    ('preprocessor', preprocessor),\\n    ('classifier', DecisionTreeClassifier(random_state=42))\\n])\\n\\n# Define a simplified param_grid\\nparam_grid = {\\n    'classifier__criterion': ['gini', 'entropy'],\\n    'classifier__max_depth': [None, 10, 20],\\n    'classifier__min_samples_split': [2, 5, 10],\\n    'classifier__min_samples_leaf': [1, 2, 4]\\n}\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Encode categorical variables\n",
    "categorical_features = ['Location', 'Cross_Street', 'Date_Reported', 'Date_Occurred', 'Area_Name', 'Part 1-2', 'Modus_Operandi', 'Victim_Sex', 'Victim_Descent', 'Premise_Description', 'Weapon_Description', 'Status', 'Status_Description']\n",
    "numeric_features = ['Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', 'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code']\n",
    "\n",
    "# Create preprocessing pipelines for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the model pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a simplified param_grid\n",
    "param_grid = {\n",
    "    'classifier__criterion': ['gini', 'entropy'],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc4e4e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.859268Z",
     "iopub.status.busy": "2024-07-30T11:29:20.858868Z",
     "iopub.status.idle": "2024-07-30T11:29:20.866563Z",
     "shell.execute_reply": "2024-07-30T11:29:20.865240Z"
    },
    "papermill": {
     "duration": 0.019871,
     "end_time": "2024-07-30T11:29:20.869374",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.849503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Perform Randomized Search with reduced iterations and parallel processing\\nrandom_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=5, scoring=\\'accuracy\\', n_jobs=-1, random_state=42)\\nrandom_search.fit(train.drop(columns=[\\'Crime_Category\\']), train[\\'Crime_Category\\'])\\n\\n# Get the best parameters and the best score\\nbest_params = random_search.best_params_\\nbest_score = random_search.best_score_\\n\\nprint(f\"Best parameters: {best_params}\")\\nprint(f\"Best cross-validation score: {best_score:.4f}\")\\n\\n# Train the model with best parameters\\nbest_model = random_search.best_estimator_\\nbest_model.fit(train.drop(columns=[\\'Crime_Category\\']), train[\\'Crime_Category\\'])\\n\\n# Make predictions on the test set\\ny_pred_test = best_model.predict(test)'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Perform Randomized Search with reduced iterations and parallel processing\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(train.drop(columns=['Crime_Category']), train['Crime_Category'])\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best cross-validation score: {best_score:.4f}\")\n",
    "\n",
    "# Train the model with best parameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_model.fit(train.drop(columns=['Crime_Category']), train['Crime_Category'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = best_model.predict(test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac6289ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.887306Z",
     "iopub.status.busy": "2024-07-30T11:29:20.886916Z",
     "iopub.status.idle": "2024-07-30T11:29:20.894785Z",
     "shell.execute_reply": "2024-07-30T11:29:20.893512Z"
    },
    "papermill": {
     "duration": 0.019638,
     "end_time": "2024-07-30T11:29:20.897364",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.877726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\n# Assuming you have already loaded and preprocessed your dataset as per previous steps\\n\\n# Define X_train and y_train\\nX_train = train.drop(columns=[\\'Crime_Category\\'])\\ny_train = train[\\'Crime_Category\\']\\n\\n# Assuming categorical_features is a list of categorical column names\\n# One-hot encode categorical features\\nX_train = pd.get_dummies(X_train, columns=categorical_features)\\n\\n# Define and train the Decision Tree model with best parameters\\ndt_classifier = DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=4, max_depth=10, criterion=\\'entropy\\', random_state=42)\\ndt_classifier.fit(X_train, y_train)\\n\\n# Predict on training set\\ny_train_pred = dt_classifier.predict(X_train)\\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\\nprint(f\"Training Set Accuracy: {train_accuracy:.4f}\")'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have already loaded and preprocessed your dataset as per previous steps\n",
    "\n",
    "# Define X_train and y_train\n",
    "X_train = train.drop(columns=['Crime_Category'])\n",
    "y_train = train['Crime_Category']\n",
    "\n",
    "# Assuming categorical_features is a list of categorical column names\n",
    "# One-hot encode categorical features\n",
    "X_train = pd.get_dummies(X_train, columns=categorical_features)\n",
    "\n",
    "# Define and train the Decision Tree model with best parameters\n",
    "dt_classifier = DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=4, max_depth=10, criterion='entropy', random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training set\n",
    "y_train_pred = dt_classifier.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Set Accuracy: {train_accuracy:.4f}\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abe27e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.915263Z",
     "iopub.status.busy": "2024-07-30T11:29:20.914875Z",
     "iopub.status.idle": "2024-07-30T11:29:20.921912Z",
     "shell.execute_reply": "2024-07-30T11:29:20.920799Z"
    },
    "papermill": {
     "duration": 0.018694,
     "end_time": "2024-07-30T11:29:20.924274",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.905580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Make predictions on the test set\\ntest_predictions = best_model.predict(test)\\n\\n# Create a sequential ID column for the test set\\ntest[\\'ID\\'] = test.index + 1  # Creates IDs starting from 1\\n\\n# Create a DataFrame with the predictions\\nsubmission = pd.DataFrame({\\n    \\'ID\\': test[\\'ID\\'],\\n    \\'Crime_Category\\': test_predictions\\n})\\n\\n# Save the submission file\\nsubmission.to_csv(\\'submission.csv\\', index=False)\\n\\n# Confirm the file has been saved\\nprint(\"Submission file saved successfully!\")'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Make predictions on the test set\n",
    "test_predictions = best_model.predict(test)\n",
    "\n",
    "# Create a sequential ID column for the test set\n",
    "test['ID'] = test.index + 1  # Creates IDs starting from 1\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test['ID'],\n",
    "    'Crime_Category': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Confirm the file has been saved\n",
    "print(\"Submission file saved successfully!\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b06792c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.942299Z",
     "iopub.status.busy": "2024-07-30T11:29:20.941877Z",
     "iopub.status.idle": "2024-07-30T11:29:20.946614Z",
     "shell.execute_reply": "2024-07-30T11:29:20.945526Z"
    },
    "papermill": {
     "duration": 0.016678,
     "end_time": "2024-07-30T11:29:20.949052",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.932374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b13be3",
   "metadata": {
    "papermill": {
     "duration": 0.007729,
     "end_time": "2024-07-30T11:29:20.965197",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.957468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dad8c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:20.983518Z",
     "iopub.status.busy": "2024-07-30T11:29:20.983091Z",
     "iopub.status.idle": "2024-07-30T11:29:41.815291Z",
     "shell.execute_reply": "2024-07-30T11:29:41.813928Z"
    },
    "papermill": {
     "duration": 20.844508,
     "end_time": "2024-07-30T11:29:41.817845",
     "exception": false,
     "start_time": "2024-07-30T11:29:20.973337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/crime-cast-forecasting-crime-categories/sample.csv\n",
      "/kaggle/input/crime-cast-forecasting-crime-categories/train.csv\n",
      "/kaggle/input/crime-cast-forecasting-crime-categories/test.csv\n",
      "Sample Submission Columns: Index(['ID', 'Crime_Category ', 'Unnamed: 2'], dtype='object')\n",
      "Train Columns: Index(['Location', 'Cross_Street', 'Latitude', 'Longitude', 'Date_Reported',\n",
      "       'Date_Occurred', 'Time_Occurred', 'Area_ID', 'Area_Name',\n",
      "       'Reporting_District_no', 'Part 1-2', 'Modus_Operandi', 'Victim_Age',\n",
      "       'Victim_Sex', 'Victim_Descent', 'Premise_Code', 'Premise_Description',\n",
      "       'Weapon_Used_Code', 'Weapon_Description', 'Status',\n",
      "       'Status_Description', 'Crime_Category'],\n",
      "      dtype='object')\n",
      "Test Columns: Index(['Location', 'Cross_Street', 'Latitude', 'Longitude', 'Date_Reported',\n",
      "       'Date_Occurred', 'Time_Occurred', 'Area_ID', 'Area_Name',\n",
      "       'Reporting_District_no', 'Part 1-2', 'Modus_Operandi', 'Victim_Age',\n",
      "       'Victim_Sex', 'Victim_Descent', 'Premise_Code', 'Premise_Description',\n",
      "       'Weapon_Used_Code', 'Weapon_Description', 'Status',\n",
      "       'Status_Description'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18/3117387827.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Date_Reported'] = pd.to_datetime(df['Date_Reported'], errors='coerce')\n",
      "/tmp/ipykernel_18/3117387827.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Date_Occurred'] = pd.to_datetime(df['Date_Occurred'], errors='coerce')\n",
      "/tmp/ipykernel_18/3117387827.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Date_Reported'] = pd.to_datetime(df['Date_Reported'], errors='coerce')\n",
      "/tmp/ipykernel_18/3117387827.py:29: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Date_Occurred'] = pd.to_datetime(df['Date_Occurred'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Accuracy: 0.7552\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load datasets\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/crime-cast-forecasting-crime-categories/sample.csv')\n",
    "\n",
    "# Print column names for debugging\n",
    "print(\"Sample Submission Columns:\", sample_submission.columns)\n",
    "print(\"Train Columns:\", train.columns)\n",
    "print(\"Test Columns:\", test.columns)\n",
    "\n",
    "# Feature Engineering: Extracting additional features\n",
    "def extract_date_features(df):\n",
    "    df['Date_Reported'] = pd.to_datetime(df['Date_Reported'], errors='coerce')\n",
    "    df['Date_Occurred'] = pd.to_datetime(df['Date_Occurred'], errors='coerce')\n",
    "    df['Report_Year'] = df['Date_Reported'].dt.year\n",
    "    df['Report_Month'] = df['Date_Reported'].dt.month\n",
    "    df['Report_Day'] = df['Date_Reported'].dt.day\n",
    "    df['Report_Hour'] = df['Date_Reported'].dt.hour\n",
    "    df['Occur_Year'] = df['Date_Occurred'].dt.year\n",
    "    df['Occur_Month'] = df['Date_Occurred'].dt.month\n",
    "    df['Occur_Day'] = df['Date_Occurred'].dt.day\n",
    "    df['Occur_Hour'] = df['Date_Occurred'].dt.hour\n",
    "    df['Time_Difference'] = (df['Date_Reported'] - df['Date_Occurred']).dt.total_seconds() / 3600.0\n",
    "    df.drop(columns=['Date_Reported', 'Date_Occurred'], inplace=True)\n",
    "    return df\n",
    "\n",
    "train = extract_date_features(train)\n",
    "test = extract_date_features(test)\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "train_imputed = pd.DataFrame(imputer.fit_transform(train.drop(columns=['Crime_Category'])), columns=train.columns.drop('Crime_Category'))\n",
    "train_imputed['Crime_Category'] = train['Crime_Category']\n",
    "test_imputed = pd.DataFrame(imputer.transform(test), columns=test.columns)\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_features = ['Location', 'Cross_Street', 'Area_Name', 'Part 1-2', 'Modus_Operandi', 'Victim_Sex', 'Victim_Descent', 'Premise_Description', 'Weapon_Description', 'Status', 'Status_Description']\n",
    "numeric_features = ['Latitude', 'Longitude', 'Time_Occurred', 'Area_ID', 'Reporting_District_no', 'Victim_Age', 'Premise_Code', 'Weapon_Used_Code', 'Report_Year', 'Report_Month', 'Report_Day', 'Report_Hour', 'Occur_Year', 'Occur_Month', 'Occur_Day', 'Occur_Hour', 'Time_Difference']\n",
    "\n",
    "# Create preprocessing pipelines for numeric and categorical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier(weights='distance', n_neighbors=13, metric='minkowski'))\n",
    "])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X = train_imputed.drop(columns=['Crime_Category'])\n",
    "y = train_imputed['Crime_Category']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "val_predictions = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "print(f\"Validation Set Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c6000a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:29:41.836609Z",
     "iopub.status.busy": "2024-07-30T11:29:41.836214Z",
     "iopub.status.idle": "2024-07-30T11:30:02.520951Z",
     "shell.execute_reply": "2024-07-30T11:30:02.519516Z"
    },
    "papermill": {
     "duration": 20.697084,
     "end_time": "2024-07-30T11:30:02.523615",
     "exception": false,
     "start_time": "2024-07-30T11:29:41.826531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(test_imputed)\n",
    "\n",
    "# Create a sequential ID column for the test set\n",
    "test['ID'] = test.index + 1  # Creates IDs starting from 1\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "submission_3 = pd.DataFrame({\n",
    "    'ID': test['ID'],\n",
    "    'Crime_Category': test_predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_3.to_csv('submission_3.csv', index=False)\n",
    "\n",
    "# Confirm the file has been saved\n",
    "print(\"Submission file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f09034cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T11:30:02.543921Z",
     "iopub.status.busy": "2024-07-30T11:30:02.543518Z",
     "iopub.status.idle": "2024-07-30T11:30:02.560057Z",
     "shell.execute_reply": "2024-07-30T11:30:02.558894Z"
    },
    "papermill": {
     "duration": 0.02967,
     "end_time": "2024-07-30T11:30:02.562805",
     "exception": false,
     "start_time": "2024-07-30T11:30:02.533135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Crime_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Violent Crimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Property Crimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Violent Crimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Property Crimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Violent Crimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>Property Crimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>Property Crimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>Violent Crimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>Crimes against Public Order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>Property Crimes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID               Crime_Category\n",
       "0        1               Violent Crimes\n",
       "1        2              Property Crimes\n",
       "2        3               Violent Crimes\n",
       "3        4              Property Crimes\n",
       "4        5               Violent Crimes\n",
       "...    ...                          ...\n",
       "4995  4996              Property Crimes\n",
       "4996  4997              Property Crimes\n",
       "4997  4998               Violent Crimes\n",
       "4998  4999  Crimes against Public Order\n",
       "4999  5000              Property Crimes\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_3"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8446444,
     "sourceId": 77420,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 45.707892,
   "end_time": "2024-07-30T11:30:03.295556",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-30T11:29:17.587664",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
